# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-LmXcuQ0RR8d1dt5FmUDaAHVWk0S1Ls

*PROYECTO 1 APRENDIZAJE AUTOMATIZADO*

**INCLUDES**
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from scipy.stats import multivariate_normal
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import validation_curve
from sklearn.metrics import accuracy_score

"""**LECTURA DE DATOS**"""

# Charge the train and test datasets
train_data = pd.read_csv("TP1_train.tsv", sep="\t")
test_data = pd.read_csv("TP1_test.tsv", sep="\t")

# Divide the characteristics (X) y the labels (y) for both train and test
X_train = train_data.iloc[:, :-1].values
y_train = train_data.iloc[:, -1].values
X_test = test_data.iloc[:, :-1].values
y_test = test_data.iloc[:, -1].values

# Normalize the characteristics
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**MCNEMAR TEST AND NORMAL TEST GIVEN IN CLASS**"""

def McNemarTest (Yt, predC1, predC2):
    rightC1 = np.array(predC1==Yt)
    rightC2 = np.array(predC2==Yt)

    e01 = np.logical_and(rightC1, np.logical_not(rightC2))
    e10 = np.logical_and(rightC2, np.logical_not(rightC1))
    e01 = sum(e01)
    e10 = sum(e10)

    if (e01+e10) == 0:
        return 0;

    return ((np.abs(e01-e10)-1)**2) / (e01 + e10)

def NormalTest (Yt, pred):
    X = sum(Yt != pred);
    N = Yt.shape[0];
    std_deviation = np.sqrt(N*(X/N)*(1-(X/N)))
    return X - (std_deviation*1.96), X + (std_deviation*1.96)

"""**KNN**"""

class KNNScratch:
    def __init__(self, metric='euclidean', n_neighbor=3):
        self.metric = metric
        self.points = []
        self.n_neighbors=n_neighbor
    # mapping for the distances
    mapeo_normas = {
        'euclidean': 2,
        'manhattan': 1,
        'infinity': np.inf,
    }

    def fit(self, X, y):
        if len(X) != len(y):
            raise ValueError("Error: X and Y must have the same length")
        self.points = [(X[i], y[i]) for i in range(len(X))]

    def predict(self, X):
        lista = np.array([])
        for x in X:
            # Caclulate the distances to the points
            topN = sorted([(np.linalg.norm(point[0] - x, self.mapeo_normas[self.metric.lower()]), point[1]) for point in self.points], key=lambda pos: pos[0])[:self.n_neighbors]
            y_top = max(topN, key=lambda pos: pos[1])[1]
            lista = np.append(lista, y_top)
        return lista

    def score(self, y_test, y_hat):
        return np.sum(y_test == y_hat) / len(y_test)

    # Maybe use the gridSearchCV
    def grid_search(self, X_train, y_train, X_val, y_val, k_values):
        best_score = -1
        best_k = None
        for k in k_values:
            self.n_neighbors = k
            self.fit(X_train, y_train)
            y_pred = self.predict(X_val)
            score = self.score(y_val, y_pred)
            if score > best_score:
                best_score = score
                best_k = k
        # Before the return we must save the best number of neighbors (for the predict in the future)
        self.n_neighbors = best_k
        return best_k, best_score

"""looking for the best n_neightboors"""

k_values = [1, 3, 5, 7, 9]  # Possible k_values to try
# Arrrays to safe the scores
k_scores = []
k_true_errors = []

# Create a model with that k value
for k in k_values:
  knn = KNNScratch(n_neighbor=k)
  knn.fit(X_train, y_train)
  y_pred_knn= knn.predict(X_test)
  score_knn = knn.score(y_test, y_pred_knn)
  # Estimate the true error
  true_error = 1 - accuracy_score(y_test, y_pred_knn)
  #adding the results to the arrays
  k_scores.append(score_knn)
  k_true_errors.append(true_error)

# Graph of the results
plt.figure(figsize=(10, 6))
plt.plot(k_values, k_scores, marker='o', label='k_scores')
plt.plot(k_values, k_true_errors, marker='x', label='k_true_error')
plt.xlabel('k_values')
plt.ylabel('Scores/Error')
plt.title('Scores y True Error para diferentes valores de k')
plt.xticks(k_values)
plt.legend()
plt.grid(True)
plt.show()

"""As we are looking for the best score and the smallest true error, we can easily observe that the k value that optimizes the classifier is k=1. So later on we will be working  with this one to compare with the rest of classifiers

Additionaly, I created a grid_search method to automatize the process of optimizating the classifiers
"""

# Supposing that we have X_train, y_train, X_val, y_val defined

k_values = [1, 3, 5, 7, 9]  # Possible k_values to try
knn = KNNScratch()
best_k, best_score = knn.grid_search(X_train, y_train, X_test, y_test, k_values)
print("Best k:", best_k)
print("Best score:", best_score)

#now predicting with the best_k (as was saved in n_neigboors during the grid_search)
y_pred_knn = knn.predict(X_test)

# Calculate the accuracy score (you can use other metrics as well)
accuracy = accuracy_score(y_test, y_pred_knn)

# Estimate the true error
true_error = 1 - accuracy
print("Estimated True Error:", true_error)

# Calculate the margin of error
print("Range of expected errors (95% confidence interval):", NormalTest(y_test, y_pred_knn))

"""**LDA**"""

class LDAScratch:
    def __init__(self):
        self.mean_vectors = {}
        self.cov_matrix = None
        self.overall_mean = None
        self.class_labels = None

    def fit(self, X, y):
        self.class_labels = np.unique(y)
        self.overall_mean = np.mean(X, axis=0)
        for label in self.class_labels:
            self.mean_vectors[label] = np.mean(X[y == label], axis=0)
        # Calculate the disspersion matrix (Sb)
        Sb = np.zeros((X.shape[1], X.shape[1]))
        for label in self.class_labels:
            n = X[y == label].shape[0]
            mean_diff = (self.mean_vectors[label] - self.overall_mean).reshape(-1, 1)
            Sb += n * mean_diff.dot(mean_diff.T)
        # Calculate the dispersion matrix of the classes (Sw)
        Sw = np.zeros((X.shape[1], X.shape[1]))
        for label in self.class_labels:
            mean_vec = self.mean_vectors[label].reshape(-1, 1)
            class_scatter = np.zeros((X.shape[1], X.shape[1]))
            for row in X[y == label]:
                row = row.reshape(-1, 1)
                mean_diff = row - mean_vec
                class_scatter += mean_diff.dot(mean_diff.T)
            Sw += class_scatter
        # Calculate autovalues and autovectors
        eigvals, eigvecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))
        # Ordering the autovalues and autovectors
        sorted_indices = np.argsort(eigvals)[::-1]
        self.eigvecs_sorted = eigvecs[:, sorted_indices]

    def transform(self, X, n_components):
        return X.dot(self.eigvecs_sorted[:, :n_components])

    def predict(self, X_test):
        X_test_transformed = self.transform(X_test, len(self.class_labels) - 1)
        y_pred = []
        for x in X_test_transformed:
            distances = [np.linalg.norm(x - self.mean_vectors[label]) for label in self.class_labels]
            y_pred.append(self.class_labels[np.argmin(distances)])
        return np.array(y_pred)

    def score(self, y_true, y_pred):
        return np.mean(y_true == y_pred)

# Initialize the LDA
lda_scratch = LDAScratch()

# Train the LDA classifier
lda_scratch.fit(X_train, y_train)

# Once trained, predict with the test values
y_pred_lda = lda_scratch.predict(X_test)

# Calculate the prediction precission
accuracy_lda_scratch = lda_scratch.score(y_test, y_pred_lda)
print("Accuracy (Custom LDA):", accuracy_lda_scratch)

#considering we have the predict done before

# Calculate the accuracy score (you can use other metrics as well)
accuracy = accuracy_score(y_test, y_pred_lda)

# Estimate the true error
true_error = 1 - accuracy
print("Estimated True Error (custom LDA):", true_error)

print("Range of expected errors (95% confidence interval):", NormalTest(y_test, y_pred_lda))

"""**QDA**"""

class QDAScratch:
    def __init__(self):
        self.mean_vectors = {}
        self.cov_matrices = {}
        self.class_labels = None

    def fit(self, X, y):
        self.class_labels = np.unique(y)
        for label in self.class_labels:
            self.mean_vectors[label] = np.mean(X[y == label], axis=0)
            self.cov_matrices[label] = np.cov(X[y == label], rowvar=False)

    def predict(self, X_test):
        y_pred = []
        for x in X_test:
            posteriors = []
            for label in self.class_labels:
                # Calculate the density function of multivariable probability for the class
                mvn = multivariate_normal(mean=self.mean_vectors[label], cov=self.cov_matrices[label])
                posterior = mvn.pdf([x])  # Pasar x como una lista
                posteriors.append(posterior)
            # The class with highest probability will be the designated one
            y_pred.append(self.class_labels[np.argmax(posteriors)])
        return np.array(y_pred)

    def score(self, y_true, y_pred):
        return np.mean(y_true == y_pred)

qda_scratch = QDAScratch()

# TRin the QDA classifier
qda_scratch.fit(X_train, y_train)

# Realize the prediction for the test dataset
y_pred_qda = qda_scratch.predict(X_test)

# Calculate the score in the predictions
accuracy_qda_scratch = qda_scratch.score(y_test, y_pred_qda)
print("Accuracy (Custom QDA):", accuracy_qda_scratch)

#considering we have the predict done before

# Calculate the accuracy score (you can use other metrics as well)
accuracy = accuracy_score(y_test, y_pred_qda)

# Estimate the true error
true_error = 1 - accuracy
print("Estimated True Error (custom QDA):", true_error)

# Calculate the range
print("Range of expected errors (95% confidence interval):", NormalTest(y_test, y_pred_qda))

"""**GAUSSIAN NAIVE BAYES**"""

# Create Naive Bayes Gaussiano classifier
nb_classifier = GaussianNB()

# Train the classificator
nb_classifier.fit(X_train, y_train)

# Predict the labels for the classifier
y_pred_gaussian = nb_classifier.predict(X_test)

# Calculate the model score
score = nb_classifier.score(X_test, y_test)
print("El puntaje del modelo Naive Bayes Gaussiano es:", score)

#considering we have the predict done before

# Calculate the accuracy score (you can use other metrics as well)
accuracy = accuracy_score(y_test, y_pred_gaussian)

# Estimate the true error
true_error = 1 - accuracy
print("Estimated True Error (GNB):", true_error)

# Calculate the margin of error
print("Range of expected errors (95% confidence interval):", NormalTest(y_test, y_pred_gaussian))

"""**SUPPORT VECTOR MACHINE**

We will start looking for the best gamma value, using C=1
"""

#possible gammas to work with
gammas = [0.001, 0.01, 0.1, 1, 10]
#arrays to save the results
svc_scores = []
svc_true_errors = []
for gamma in gammas:
  # Create the SVC classifier
  svc_classifier = SVC(C=1, gamma=gamma)

  #train and test of the model
  svc_classifier.fit(X_train, y_train)
  y_pred_svc = svc_classifier.predict(X_test)
  svc_score = svc_classifier.score(X_test, y_test)
  true_error = 1 - accuracy_score(y_test, y_pred_svc)

  #save the values in the arrays
  svc_scores.append(svc_score)
  svc_true_errors.append(true_error)

# Graph of the results
plt.figure(figsize=(10, 6))
plt.semilogx(gammas, svc_scores, marker='o', label='svc_scores')
plt.semilogx(gammas, svc_true_errors, marker='x', label='svc_true_error')
plt.xlabel('gamma_values')
plt.ylabel('Scores/Error')
plt.title('Scores y True Error para diferentes valores de svc')
plt.xticks(gammas)
plt.legend()
plt.grid(True)
plt.show()

"""In a similar way as in the knn but this time for gamma, we are looking for a small error and the highest score possible, clearly we can observe that this occures for gamma = 1. We will be using this classifier with C=1 and gamma=1 later on when comparing the classifiers

Additionaly, in python the sklearn library includes a GridSearchCV to actomatize this method, so I implemented it bellow, in order to contrast with my results
"""

# Define the parameters to search for the best gamma
# Definine the parameters
param_grid = {'gamma': [0.001, 0.01, 0.1, 1, 10]}

# Create the SVC classifier
svc_classifier = SVC(C=1)

# Use cross validation to search for the best gamma
grid_search = GridSearchCV(svc_classifier, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best estimator (the SVM witht the best gamma)
best_svc = grid_search.best_estimator_
best_gamma = grid_search.best_params_['gamma']

print("Mejor gamma encontrado:", best_gamma)

#now using the best gamma
y_pred_svc = best_svc.predict(X_test)

# Evaluate the model for the classifier
test_score = best_svc.score(X_test, y_test)
print("Puntaje del modelo en el conjunto de prueba:", test_score)

#considering we have the predict done before

# Calculate the accuracy score (you can use other metrics as well)
accuracy = accuracy_score(y_test, y_pred_svc)
true_error = 1 - accuracy
print("Estimated True Error (SVC):", true_error)

print("Range of expected errors (95% confidence interval):", NormalTest(y_test, y_pred_svc))

"""**extra optimization of C and gamma**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Possible gammas and Cs to work with
gammas = [0.001, 0.01, 0.1, 1, 10]
Cs = [0.1, 1, 10, 100]

# Arrays to save the results
svc_scores = np.zeros((len(gammas), len(Cs)))
svc_true_errors = np.zeros((len(gammas), len(Cs)))

for i, gamma in enumerate(gammas):
    for j, C in enumerate(Cs):
        # Create the SVC classifier
        svc_classifier = SVC(C=C, gamma=gamma)

        # Train and test of the model
        svc_classifier.fit(X_train, y_train)
        y_pred_svc = svc_classifier.predict(X_test)
        svc_score = svc_classifier.score(X_test, y_test)
        true_error = 1 - accuracy_score(y_test, y_pred_svc)

        # Save the values in the arrays
        svc_scores[i, j] = svc_score
        svc_true_errors[i, j] = true_error

# Plot of the results
plt.figure(figsize=(12, 6))

# Plot scores
plt.subplot(1, 2, 1)
for j, C in enumerate(Cs):
    plt.semilogx(gammas, svc_scores[:, j], marker='o', label=f'C={C}')
plt.xlabel('Gamma Values')
plt.ylabel('Scores')
plt.title('Scores for different Gamma Values')
plt.legend()
plt.grid(True)

# Plot true errors
plt.subplot(1, 2, 2)
for j, C in enumerate(Cs):
    plt.semilogx(gammas, svc_true_errors[:, j], marker='x', label=f'C={C}')
plt.xlabel('Gamma Values')
plt.ylabel('True Error')
plt.title('True Error for different Gamma Values')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

perfect_svc = SVC(C=0.1, gamma=1)
perfect_svc.fit(X_train, y_train)
perfect_svc.predict(X_test)

# Evaluate the model for the classifier
test_score = perfect_svc.score(X_test, y_test)
print("Puntaje del modelo en el conjunto de prueba:", test_score)

"""We can observe that the C=0.1 gives a more optimal score, but looking that the graph we can see that the difference obtained is very low (around a 0.01 difference).

**COMPARING CLASSIFIERS W MCNEMAR TEST**

As the scores and true error of the classifiers are really close, we should compare them though the mcnemar test
"""

# Comparing KNN vs LDA (considering the predict have been calculated before)
print("results McNemar test KNN vs LDA: ", McNemarTest(y_test, y_pred_knn, y_pred_lda))
# As the result of the test is higher than the value given in class (mcnemar >= 3.84), we can conclude that there's a big difference between the test, meaning this there's a better one.
# To know which is the best one, we should compare the normal test obtained before and choose the one with the smallest interval (as that means it has the smalles standart desviation)
# As the KNN has a width interval of around 30 and the LDA of 60 we conclude that the best classifier is the KNN

# Compararing KNN and QDA
print("results McNemar test KNN vs QDA: ", McNemarTest(y_test, y_pred_knn, y_pred_qda))
# As the value is bellow 3.84 we can't conclude that one test is better than the other

# Compararing KNN and GNV
print("results McNemar test KNN vs Gauissian: ", McNemarTest(y_test, y_pred_knn, y_pred_gaussian))
# As the result of the test is higher than the value given in class (mcnemar >= 3.84), we can conclude that there's a big difference between the test, meaning this there's a better one.
# To know which is the best one, we should compare the normal test obtained before and choose the one with the smallest interval (as that means it has the smalles standart desviation)
# As the KNN has a width interval of 34 and the Gaussian of 41 we will consider the KNN as the better classifier for this dataset

# Compararing KNN and SVC
print("results McNemar test KNN vs SVC: ", McNemarTest(y_test, y_pred_knn, y_pred_svc))
# As the result of the test is higher than the value given in class (mcnemar >= 3.84), we can conclude that there's a big difference between the test, meaning this there's a better one.
# To know which is the best one, we should compare the normal test obtained before and choose the one with the smallest interval (as that means it has the smalles standart desviation)
# As the KNN has a width interval of 34 and the SVC of 27 we will consider the SVC as the better classifier for this dataset
# Having compared all the classifier between each other (we didn't compare SVC with Gaussian for example, but we did with KNN which resulted to be better), we reached to the conclusion that the SVC is the best classifier between the five for this specific dataset